services:
  secondbrain:
    image: ghcr.io/mcasperson/secondbrain
    restart: always
    ports:
      - "8080:8080"
      - "8181:8181"
    environment:
      # These values must be updated to use the details of your own Slack App
      # if you wish to use the Oauth login flow: https://api.slack.com/apps
      SB_SLACK_CLIENTID: ${SB_SLACK_CLIENTID}
      SB_SLACK_CLIENTSECRET: ${SB_SLACK_CLIENTSECRET}

      # These values must be updated to use the details of your own Google App
      SB_GOOGLE_CLIENTID: ${SB_GOOGLE_CLIENTID}
      SB_GOOGLE_CLIENTSECRET: ${SB_GOOGLE_CLIENTSECRET}
      SB_GOOGLE_REDIRECTURL: ${SB_GOOGLE_REDIRECTURL}

      # You should change this to a random value
      SB_ENCRYPTION_PASSWORD: 123456789
      # This value refers to the sibling container
      SB_OLLAMA_URL: http://ollama:11434
      # Set this to true to enable debug logging
      SB_TOOLS_DEBUG: false
      # Define the model to use by SecondBrain.
      # Try "llama3.1" for a larger model, or "llama3.2" for a smaller model.
      # Remember, each model needs to be pulled with the command below.
      # docker exec secondbrain-ollama-1 ollama pull <model name>
      # for example
      # docker exec secondbrain-ollama-1 ollama pull llama3.2
      SB_OLLAMA_MODEL: llama3.2
      # The model used to select a tool should always been llama3.1 or llama3.2
      # as these models have been trained to select tools. The model used here
      # must also be pulled:
      # docker exec secondbrain-ollama-1 ollama pull llama3.1
      SB_OLLAMA_TOOLMODEL: llama3.1
      # This aids in debugging
      SB_EXCEPTIONS_PRINTSTACKTRACE: false
      # This value is used to cache the results of various API calls
      SB_CACHE_PATH: /cache
      # Autoserver is not supported by docker
      SB_CACHE_AUTOSERVER: false
    pull_policy: always
    volumes:
      - cache:/cache

  ollama:
    image: ollama/ollama
    restart: always
    pull_policy: always
    volumes:
      - ollama:/root/.ollama
    # Uncomment these lines to enable GPU support
    # You'll also need to install the NVIDIA Container Toolkit on your host
    # https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html
    # Test that the GPU is available from Docker by running
    # docker run --rm -it --gpus=all nvcr.io/nvidia/k8s/cuda-sample:nbody nbody -gpu -benchmark
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [ gpu ]
volumes:
  ollama:
  cache:
